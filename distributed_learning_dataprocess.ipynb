{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dask-scheduler'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m client \u001b[39m=\u001b[39m vineyard\u001b[39m.\u001b[39mconnect()\n\u001b[1;32m      9\u001b[0m \u001b[39m# launch dask scheduler and worker\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m dask_scheduler \u001b[39m=\u001b[39m sp\u001b[39m.\u001b[39;49mPopen([\u001b[39m'\u001b[39;49m\u001b[39mdask-scheduler\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m--host\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mlocalhost\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     11\u001b[0m dask_worker \u001b[39m=\u001b[39m sp\u001b[39m.\u001b[39mPopen([\u001b[39m'\u001b[39m\u001b[39mdask-worker\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtcp://localhost:8786\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_mode:\n\u001b[1;32m    968\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[1;32m    969\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m--> 971\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m    972\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m    973\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m    974\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m    975\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m    976\u001b[0m                         errread, errwrite,\n\u001b[1;32m    977\u001b[0m                         restore_signals,\n\u001b[1;32m    978\u001b[0m                         gid, gids, uid, umask,\n\u001b[1;32m    979\u001b[0m                         start_new_session)\n\u001b[1;32m    980\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m    982\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdin, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1863\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     \u001b[39mif\u001b[39;00m errno_num \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1862\u001b[0m         err_msg \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[39mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1864\u001b[0m \u001b[39mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dask-scheduler'"
     ]
    }
   ],
   "source": [
    "import vineyard\n",
    "import subprocess as sp\n",
    "\n",
    "# socket = '/tmp/vineyard.sock'\n",
    "\n",
    "# launch local vineyardd\n",
    "client = vineyard.connect()\n",
    "\n",
    "# launch dask scheduler and worker\n",
    "dask_scheduler = sp.Popen(['dask-scheduler', '--host', 'localhost'])\n",
    "dask_worker = sp.Popen(['dask-worker', 'tcp://localhost:8786'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Dask dataframe requirements are not installed.\n\nPlease either conda or pip install as follows:\n\n  conda install dask                     # either conda install\n  python -m pip install \"dask[dataframe]\" --upgrade  # or python -m pip install",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dask/dataframe/__init__.py:97\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m _dask_expr_enabled():\n\u001b[0;32m---> 97\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mdask_expr\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mdd\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[39m# trigger loading of dask-expr which will in-turn import dask.dataframe and run remainder\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[39m# of this module's init updating attributes to be dask-expr\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[39m# note: needs reload, in case dask-expr imported before dask.dataframe; works fine otherwise\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dask_expr'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataframe\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mdd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m raw_data \u001b[39m=\u001b[39m dd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mcovtype.data\u001b[39m\u001b[39m'\u001b[39m, header\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dask/dataframe/__init__.py:110\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    104\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    105\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDask dataframe requirements are not installed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease either conda or pip install as follows:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m  conda install dask                     # either conda install\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m  python -m pip install \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdask[dataframe]\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m --upgrade  # or python -m pip install\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    109\u001b[0m     )\n\u001b[0;32m--> 110\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m _dask_expr_enabled():\n\u001b[1;32m    114\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: Dask dataframe requirements are not installed.\n\nPlease either conda or pip install as follows:\n\n  conda install dask                     # either conda install\n  python -m pip install \"dask[dataframe]\" --upgrade  # or python -m pip install"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "raw_data = dd.read_csv('covtype.data', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m soil_type_values \u001b[39m=\u001b[39m [\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msoil_type_\u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m40\u001b[39m)]\n\u001b[1;32m     10\u001b[0m wilderness_area_values \u001b[39m=\u001b[39m [\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39marea_type_\u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m4\u001b[39m)]\n\u001b[0;32m---> 12\u001b[0m soil_type \u001b[39m=\u001b[39m raw_data\u001b[39m.\u001b[39mloc[:, \u001b[39m14\u001b[39m:\u001b[39m53\u001b[39m]\u001b[39m.\u001b[39mapply(\n\u001b[1;32m     13\u001b[0m     \u001b[39mlambda\u001b[39;00m x: soil_type_values[\u001b[39m0\u001b[39m::\u001b[39m1\u001b[39m][x\u001b[39m.\u001b[39mto_numpy()\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m wilderness_area \u001b[39m=\u001b[39m raw_data\u001b[39m.\u001b[39mloc[:, \u001b[39m10\u001b[39m:\u001b[39m13\u001b[39m]\u001b[39m.\u001b[39mapply(\n\u001b[1;32m     16\u001b[0m     \u001b[39mlambda\u001b[39;00m x: wilderness_area_values[\u001b[39m0\u001b[39m::\u001b[39m1\u001b[39m][x\u001b[39m.\u001b[39mto_numpy()\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m CSV_HEADER \u001b[39m=\u001b[39m [\n\u001b[1;32m     20\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mElevation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mAspect\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mCover_Type\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'raw_data' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The two categorical features in the dataset are binary-encoded.\n",
    "We will convert this dataset representation to the typical representation, where each\n",
    "categorical feature is represented as a single integer value.\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "soil_type_values = [f\"soil_type_{idx+1}\" for idx in range(40)]\n",
    "wilderness_area_values = [f\"area_type_{idx+1}\" for idx in range(4)]\n",
    "\n",
    "soil_type = raw_data.loc[:, 14:53].apply(\n",
    "    lambda x: soil_type_values[0::1][x.to_numpy().nonzero()[0][0]], axis=1\n",
    ")\n",
    "wilderness_area = raw_data.loc[:, 10:13].apply(\n",
    "    lambda x: wilderness_area_values[0::1][x.to_numpy().nonzero()[0][0]], axis=1\n",
    ")\n",
    "\n",
    "CSV_HEADER = [\n",
    "    \"Elevation\",\n",
    "    \"Aspect\",\n",
    "    \"Slope\",\n",
    "    \"Horizontal_Distance_To_Hydrology\",\n",
    "    \"Vertical_Distance_To_Hydrology\",\n",
    "    \"Horizontal_Distance_To_Roadways\",\n",
    "    \"Hillshade_9am\",\n",
    "    \"Hillshade_Noon\",\n",
    "    \"Hillshade_3pm\",\n",
    "    \"Horizontal_Distance_To_Fire_Points\",\n",
    "    \"Wilderness_Area\",\n",
    "    \"Soil_Type\",\n",
    "    \"Cover_Type\",\n",
    "]\n",
    "\n",
    "data = dd.concat(\n",
    "    [raw_data.loc[:, 0:9], wilderness_area, soil_type, raw_data.loc[:, 54]],\n",
    "    axis=1,\n",
    "    ignore_index=True,\n",
    ")\n",
    "data.columns = CSV_HEADER\n",
    "\n",
    "# Convert the target label indices into a range from 0 to 6 (there are 7 labels in total).\n",
    "data[\"Cover_Type\"] = data[\"Cover_Type\"] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vineyard.contrib.dask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mvineyard\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mvineyard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbuilder\u001b[39;00m \u001b[39mimport\u001b[39;00m builder_context\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mvineyard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontrib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdask\u001b[39;00m \u001b[39mimport\u001b[39;00m register_dask_types\n\u001b[1;32m      5\u001b[0m \u001b[39mwith\u001b[39;00m builder_context() \u001b[39mas\u001b[39;00m builder:\n\u001b[1;32m      6\u001b[0m     register_dask_types(builder, \u001b[39mNone\u001b[39;00m) \u001b[39m# register dask builders\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vineyard.contrib.dask'"
     ]
    }
   ],
   "source": [
    "import vineyard\n",
    "from vineyard.core.builder import builder_context\n",
    "from vineyard.contrib.dask.dask import register_dask_types\n",
    "\n",
    "with builder_context() as builder:\n",
    "    register_dask_types(builder, None) # register dask builders\n",
    "    gdf_id = client.put(data, dask_scheduler='tcp://localhost:8786')\n",
    "    print(gdf_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vineyard.contrib.ml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mvineyard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mresolver\u001b[39;00m \u001b[39mimport\u001b[39;00m resolver_context\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mvineyard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontrib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m register_tf_types\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_dataset_from_vineyard\u001b[39m(object_id, batch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m      5\u001b[0m     \u001b[39mwith\u001b[39;00m resolver_context() \u001b[39mas\u001b[39;00m resolver:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vineyard.contrib.ml'"
     ]
    }
   ],
   "source": [
    "from vineyard.core.resolver import resolver_context\n",
    "from vineyard.contrib.ml.tensorflow import register_tf_types\n",
    "\n",
    "def get_dataset_from_vineyard(object_id, batch_size, shuffle=False):\n",
    "    with resolver_context() as resolver:\n",
    "        register_tf_types(None, resolver) # register tf resolvers\n",
    "        ds = vineyard.connect().get(object_id, label=TARGET_FEATURE_NAME) # specify the label column\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(len(ds))\n",
    "\n",
    "    len_test = int(len(ds) * 0.15)\n",
    "    test_dataset = ds.take(len_test).batch(batch_size)\n",
    "    train_dataset = ds.skip(len_test).batch(batch_size)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'horovod'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mhorovod\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mhvd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_experiment\u001b[39m(model):\n\u001b[1;32m      5\u001b[0m     hvd\u001b[39m.\u001b[39minit()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'horovod'"
     ]
    }
   ],
   "source": [
    "import horovod.keras as hvd\n",
    "\n",
    "def run_experiment(model):\n",
    "\n",
    "    hvd.init()\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=hvd.DistributedOptimizer(keras.optimizers.Adam(learning_rate=learning_rate)),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        # Horovod: broadcast initial variable states from rank 0 to all other processes.\n",
    "        # This is necessary to ensure consistent initialization of all workers when\n",
    "        # training is started with random weights or restored from a checkpoint.\n",
    "        hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "    ]\n",
    "\n",
    "    train_dataset, test_dataset = get_dataset_from_vineyard(gdf_id, batch_size, shuffle=True)\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    history = model.fit(train_dataset, epochs=num_epochs, callbacks=callbacks)\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    _, accuracy = model.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_FEATURE_NAME = \"Cover_Type\"\n",
    "\n",
    "TARGET_FEATURE_LABELS = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n",
    "\n",
    "NUMERIC_FEATURE_NAMES = [\n",
    "    \"Aspect\",\n",
    "    \"Elevation\",\n",
    "    \"Hillshade_3pm\",\n",
    "    \"Hillshade_9am\",\n",
    "    \"Hillshade_Noon\",\n",
    "    \"Horizontal_Distance_To_Fire_Points\",\n",
    "    \"Horizontal_Distance_To_Hydrology\",\n",
    "    \"Horizontal_Distance_To_Roadways\",\n",
    "    \"Slope\",\n",
    "    \"Vertical_Distance_To_Hydrology\",\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"Soil_Type\": soil_type_values,\n",
    "    \"Wilderness_Area\": wilderness_area_values,\n",
    "}\n",
    "\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
    "\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "\n",
    "NUM_CLASSES = len(TARGET_FEATURE_LABELS)\n",
    "\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.1\n",
    "batch_size = 265\n",
    "num_epochs = 5\n",
    "\n",
    "hidden_units = [32, 32]\n",
    "\n",
    "\"\"\"\n",
    "## Create model inputs\n",
    "Now, define the inputs for the models as a dictionary, where the key is the feature name,\n",
    "and the value is a `keras.layers.Input` tensor with the corresponding feature shape\n",
    "and data type.\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.float32\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.string\n",
    "            )\n",
    "    return inputs\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Encode features\n",
    "We create two representations of our input features: sparse and dense:\n",
    "1. In the **sparse** representation, the categorical features are encoded with one-hot\n",
    "encoding using the `CategoryEncoding` layer. This representation can be useful for the\n",
    "model to *memorize* particular feature values to make certain predictions.\n",
    "2. In the **dense** representation, the categorical features are encoded with\n",
    "low-dimensional embeddings using the `Embedding` layer. This representation helps\n",
    "the model to *generalize* well to unseen feature combinations.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import StringLookup\n",
    "\n",
    "\n",
    "def encode_inputs(inputs, use_embedding=False):\n",
    "    encoded_features = []\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "            # Create a lookup to convert string values to an integer indices.\n",
    "            # Since we are not using a mask token nor expecting any out of vocabulary\n",
    "            # (oov) token, we set mask_token to None and  num_oov_indices to 0.\n",
    "            lookup = StringLookup(\n",
    "                vocabulary=vocabulary,\n",
    "                mask_token=None,\n",
    "                num_oov_indices=0,\n",
    "                output_mode=\"int\" if use_embedding else \"binary\",\n",
    "            )\n",
    "            if use_embedding:\n",
    "                # Convert the string input values into integer indices.\n",
    "                encoded_feature = lookup(inputs[feature_name])\n",
    "                embedding_dims = int(math.sqrt(len(vocabulary)))\n",
    "                # Create an embedding layer with the specified dimensions.\n",
    "                embedding = layers.Embedding(\n",
    "                    input_dim=len(vocabulary), output_dim=embedding_dims\n",
    "                )\n",
    "                # Convert the index values to embedding representations.\n",
    "                encoded_feature = embedding(encoded_feature)\n",
    "            else:\n",
    "                # Convert the string input values into a one hot encoding.\n",
    "                encoded_feature = lookup(tf.expand_dims(inputs[feature_name], -1))\n",
    "        else:\n",
    "            # Use the numerical features as-is.\n",
    "            encoded_feature = tf.expand_dims(inputs[feature_name], -1)\n",
    "\n",
    "        encoded_features.append(encoded_feature)\n",
    "\n",
    "    all_features = layers.concatenate(encoded_features)\n",
    "    return all_features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Experiment 1: a baseline model\n",
    "In the first experiment, let's create a multi-layer feed-forward network,\n",
    "where the categorical features are one-hot encoded.\n",
    "\"\"\"\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_baseline_model():\n",
    "    inputs = create_model_inputs()\n",
    "    features = encode_inputs(inputs)\n",
    "\n",
    "    for units in hidden_units:\n",
    "        features = layers.Dense(units)(features)\n",
    "        features = layers.BatchNormalization()(features)\n",
    "        features = layers.ReLU()(features)\n",
    "        features = layers.Dropout(dropout_rate)(features)\n",
    "\n",
    "    outputs = layers.Dense(units=NUM_CLASSES, activation=\"softmax\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "baseline_model = create_baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(baseline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "dask_worker.terminate()\n",
    "dask_scheduler.terminate()\n",
    "\n",
    "vineyard.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
